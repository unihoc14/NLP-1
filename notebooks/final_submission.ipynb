{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "This notebook serves as the final submission for the project on **Word Embeddings & RNNs** using the **CommonsenseQA dataset**. It includes:\n",
    "\n",
    "- Data loading\n",
    "- Data preprocessing (tokenization, cleaning, padding)\n",
    "- Integration of Word Embeddings (word2vec)\n",
    "- Model definition using a two-layer feedforward classifier\n",
    "- Loss function and optimizer setup\n",
    "- Experiment tracking with Weights & Biases (WandB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Setup & Installations**\n",
    "\n",
    "**Rationale:**\n",
    "- `torch`: Required for model training and tensor processing.\n",
    "- `datasets`: Used to load the CommonsenseQA dataset from Hugging Face.\n",
    "- `nltk`: Employed for tokenization and text preprocessing.\n",
    "- `gensim`: Used for loading pre-trained word embeddings (word2vec).\n",
    "- `torch.nn`: Required to define the model architecture.\n",
    "- `torch.optim`: Used for training the model with gradient updates.\n",
    "- `wandb`: Used for tracking experiments, logging metrics, and visualizing model performance.\n",
    "- `argparse`: Enables dynamic configuration of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\nlp_1\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: datasets in c:\\nlp_1\\.venv\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: nltk in c:\\nlp_1\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\nlp_1\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: wandb in c:\\nlp_1\\.venv\\lib\\site-packages (0.19.8)\n",
      "Requirement already satisfied: filelock in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: click in c:\\nlp_1\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\nlp_1\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\nlp_1\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (2.23.1)\n",
      "Requirement already satisfied: setproctitle in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: colorama in c:\\nlp_1\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: wrapt in c:\\nlp_1\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\nlp_1\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install torch datasets nltk gensim wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\nlp_1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Loading**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Follows the exact project specification from the Course Project.pdf.\n",
    "- Loads specific dataset splits for training, validation, and test using the TAU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Data Preprocessing**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Prepares raw text inputs for downstream modeling.\n",
    "- Ensures text normalization through lowercasing and punctuation removal.\n",
    "- Tokenization helps break input into word-level units for embeddings.\n",
    "- Padding standardizes input lengths for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Dataset Class (Shared)**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Provides a PyTorch-compatible wrapper around the CommonsenseQA dataset.\n",
    "- Centralizes tokenization, preprocessing, and answer label formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            self.dataset = train\n",
    "        elif split == \"validation\":\n",
    "            self.dataset = valid\n",
    "        elif split == \"test\":\n",
    "            self.dataset = test\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "        self.processed_data = self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        processed = []\n",
    "        for item in self.dataset:\n",
    "            question = preprocess_text(item[\"question\"])\n",
    "            choices = [preprocess_text(choice) for choice in item[\"choices\"][\"text\"]]\n",
    "            answer = ord(item[\"answerKey\"]) - ord('A')\n",
    "            processed.append((question, choices, answer))\n",
    "        return processed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Load Word Embeddings (word2vec)**\n",
    "\n",
    "**Rationale:**\n",
    "- I use Gensim's pre-trained Google News vectors (300D) for semantic information.\n",
    "- Pre-trained embeddings reduce the need for large training data.\n",
    "- Unknown words return a zero vector, which avoids crashes during lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0189,  0.1187, -0.0625,  0.0786, -0.0347,  0.2520,  0.0206, -0.1641,\n",
      "        -0.0212,  0.3184])\n"
     ]
    }
   ],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_word_vector(word):\n",
    "    vec = word2vec[word] if word in word2vec else torch.zeros(300)\n",
    "    return torch.tensor(vec) if not isinstance(vec, torch.Tensor) else vec\n",
    "\n",
    "print(get_word_vector(\"cold\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Feedforward Data Pipeline**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Converts token sequences into averaged embeddings.\n",
    "- Simplifies the input into fixed-size vectors.\n",
    "- Suitable for feedforward networks requiring consistent input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sequence(tokens):\n",
    "    vectors = [get_word_vector(token) for token in tokens]\n",
    "    if len(vectors) == 0:\n",
    "        return torch.zeros(300)\n",
    "    return torch.stack(vectors).mean(dim=0)\n",
    "\n",
    "def collate_fn_ffnn(batch):\n",
    "    questions, choices, answers = zip(*batch)\n",
    "    embedded_questions = torch.stack([embed_sequence(q) for q in questions])\n",
    "    embedded_choices = torch.stack([\n",
    "        torch.stack([embed_sequence(choice) for choice in choice_list])\n",
    "        for choice_list in choices\n",
    "    ])\n",
    "    return embedded_questions, embedded_choices, torch.tensor(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. LSTM Data Pipeline**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- I use full padded sequences of word embeddings for input.\n",
    "- Prepares input suitable for recurrent models (e.g., LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens):\n",
    "    return torch.stack([get_word_vector(token) for token in tokens]) if tokens else torch.zeros((1, 300))\n",
    "\n",
    "def collate_fn_lstm(batch):\n",
    "    questions, choices, answers = zip(*batch)\n",
    "    embedded_questions = torch.nn.utils.rnn.pad_sequence(\n",
    "        [embed_tokens(q) for q in questions], batch_first=True\n",
    "    )\n",
    "    embedded_choices = torch.stack([\n",
    "        torch.nn.utils.rnn.pad_sequence([embed_tokens(choice) for choice in choice_list], batch_first=True)\n",
    "        for choice_list in choices\n",
    "    ])\n",
    "    return embedded_questions, embedded_choices, torch.tensor(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Initialize WandB Tracking**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- WandB helps track hyperparameters, losses, and accuracy during training.\n",
    "- Enables experiment comparison and makes training behavior reproducible.\n",
    "- The architecture type (\"ffnn\" or \"lstm\") is also logged in the configuration to support automatic routing in data loading and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonas14\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\nlp_1\\notebooks\\wandb\\run-20250405_105126-ahdl9hpz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jonas14/commonsense_qa/runs/ahdl9hpz' target=\"_blank\">ffnn_baseline</a></strong> to <a href='https://wandb.ai/jonas14/commonsense_qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jonas14/commonsense_qa' target=\"_blank\">https://wandb.ai/jonas14/commonsense_qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jonas14/commonsense_qa/runs/ahdl9hpz' target=\"_blank\">https://wandb.ai/jonas14/commonsense_qa/runs/ahdl9hpz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"commonsense_qa\", name=\"ffnn_baseline\")\n",
    "wandb.config.update({\n",
    "    \"architecture\": \"ffnn\",\n",
    "    \"input_dim\": 300,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"output_dim\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 16\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Define get_dataloaders Function**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Automatically chooses the correct collate function based on the model architecture defined in the WandB configuration.\n",
    "- Avoids manual mode switching or hardcoding.\n",
    "- Enables consistent data access for both training and evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=None):\n",
    "    batch_size = batch_size or wandb.config[\"batch_size\"]\n",
    "    mode = wandb.config.get(\"architecture\", \"ffnn\")\n",
    "    collate = collate_fn_ffnn if mode == \"ffnn\" else collate_fn_lstm\n",
    "\n",
    "    train_dataset = CommonsenseQADataset(\"train\")\n",
    "    val_dataset = CommonsenseQADataset(\"validation\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11. Pipeline Test â€“ FFNN**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Ensures the FFNN collate function works and produces expected shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFNN] Sample question shape: torch.Size([16, 300])\n",
      "[FFNN] Sample choices shape: torch.Size([16, 5, 300])\n",
      "[FFNN] Label: tensor(4)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloaders(batch_size=wandb.config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"[FFNN] Sample question shape:\", questions.shape)\n",
    "    print(\"[FFNN] Sample choices shape:\", choices.shape)\n",
    "    print(\"[FFNN] Label:\", labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12. Pipeline Test â€“ LSTM**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Verifies padding and batching for the LSTM collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Sample question shape: torch.Size([16, 300])\n",
      "[LSTM] Sample choices shape: torch.Size([16, 5, 300])\n",
      "[LSTM] Label: tensor(2)\n",
      "Sample padded question: tensor([ 1.1320e-01,  4.9141e-02,  5.9082e-02,  1.1754e-01, -8.5754e-02,\n",
      "         1.9501e-02,  5.3986e-02, -5.8990e-02,  8.9722e-02,  4.9744e-03,\n",
      "        -2.9423e-02, -1.1263e-01, -3.2196e-03, -9.3002e-02, -1.7822e-01,\n",
      "         1.4984e-02, -3.4332e-02,  1.5143e-01,  5.9565e-02, -8.4503e-02,\n",
      "        -1.8448e-02,  3.8330e-02,  5.9113e-02, -6.1934e-02,  4.6157e-02,\n",
      "         8.7321e-03, -3.6316e-02,  3.1708e-02,  5.5115e-02,  9.7504e-02,\n",
      "        -1.6357e-02,  7.5928e-02, -5.4703e-02, -7.4402e-02,  1.4647e-02,\n",
      "        -2.3743e-02,  4.5227e-02,  2.8519e-02,  1.3540e-01,  8.9355e-02,\n",
      "         1.8975e-01, -3.1750e-02,  1.4549e-01, -9.0925e-02, -9.3536e-02,\n",
      "         2.4086e-02, -4.5959e-02,  6.1035e-05,  5.7968e-02, -1.2726e-02,\n",
      "         2.2614e-02,  1.2263e-01, -1.6464e-02, -9.0126e-02,  6.9458e-02,\n",
      "        -1.0910e-03,  7.9269e-03, -5.1460e-02,  5.6305e-02, -2.8992e-02,\n",
      "         1.5574e-02,  1.5338e-01, -1.0650e-01, -1.0714e-01, -6.5231e-02,\n",
      "         3.8727e-02,  2.1751e-02,  7.5623e-02, -4.8996e-02,  6.3416e-02,\n",
      "         1.3654e-01, -1.6068e-02,  5.6179e-02, -3.5690e-02, -1.4807e-01,\n",
      "        -8.8539e-02,  6.3887e-02,  8.5236e-02,  5.0781e-02,  2.0361e-01,\n",
      "         4.4647e-02, -7.2632e-03,  1.0141e-01, -2.7393e-02, -8.9355e-02,\n",
      "        -9.0652e-02, -6.4838e-02,  2.0349e-01,  8.3553e-02,  3.4801e-02,\n",
      "         3.5599e-02,  6.8817e-02, -8.9575e-02, -3.7880e-02, -1.2207e-03,\n",
      "        -1.6290e-01,  8.3191e-02,  5.0316e-02, -2.8137e-02, -2.9663e-02,\n",
      "        -9.2346e-02, -6.7867e-02,  3.7445e-02,  2.0177e-02, -9.6619e-02,\n",
      "        -4.3533e-02, -7.0286e-02, -6.4646e-02, -6.4163e-03, -8.2241e-02,\n",
      "         7.0152e-02,  3.7079e-03, -1.6809e-02, -3.7701e-02,  1.0411e-01,\n",
      "        -1.1459e-02,  5.9975e-02, -2.1866e-02,  7.7417e-02, -5.5237e-03,\n",
      "        -1.2263e-01, -1.1292e-02, -5.3009e-02,  1.1360e-01, -6.8680e-02,\n",
      "        -2.0974e-02, -4.1733e-02,  4.9789e-02,  1.7846e-02, -1.9646e-02,\n",
      "        -6.9565e-02, -1.6675e-01, -4.6814e-02, -7.9269e-02, -8.5796e-02,\n",
      "        -1.5283e-01,  9.6357e-02,  3.3545e-02, -6.6315e-02,  9.0630e-02,\n",
      "         5.3917e-02, -6.7589e-02,  9.1410e-02,  6.2847e-03,  3.2761e-02,\n",
      "         2.5669e-02, -5.0632e-02, -9.1431e-02, -6.1142e-02, -3.0594e-02,\n",
      "         7.4713e-02,  1.4072e-01, -1.6550e-01,  5.7404e-02,  1.5106e-03,\n",
      "         3.9856e-02, -9.3781e-02, -1.0021e-01, -2.5080e-02, -2.7557e-02,\n",
      "         2.6825e-02,  1.5652e-01,  2.0238e-02,  4.1058e-02, -1.3039e-02,\n",
      "        -1.0155e-01,  4.5921e-02, -1.1621e-01,  8.6159e-02,  3.5324e-03,\n",
      "        -1.3502e-01, -2.3819e-02,  2.9663e-02, -2.7863e-02, -4.3732e-02,\n",
      "         2.1187e-02,  9.4437e-02, -9.3384e-02,  1.9928e-02,  1.1883e-02,\n",
      "        -1.4252e-02, -6.2990e-02,  2.8111e-02, -3.9688e-02,  1.6262e-02,\n",
      "         8.6060e-03, -4.7928e-02,  9.1133e-02,  6.4545e-02,  4.1946e-02,\n",
      "         5.9967e-02,  8.9493e-02,  6.0547e-02,  1.4795e-02, -1.7349e-02,\n",
      "         4.8767e-02, -8.5348e-02,  1.9430e-02,  2.9510e-02, -4.9656e-02,\n",
      "         4.0516e-02,  7.8087e-03, -4.0070e-02, -3.4180e-03, -2.5032e-02,\n",
      "        -3.6652e-02, -5.2780e-02, -6.4606e-02, -2.4780e-02, -7.5363e-02,\n",
      "         3.1521e-02,  6.2500e-02,  3.1418e-02,  1.7258e-02, -1.6870e-01,\n",
      "        -1.3153e-02,  9.6268e-02, -3.1235e-02, -1.4513e-01,  3.7578e-02,\n",
      "        -5.0470e-02, -3.8483e-02, -5.6086e-02, -7.5402e-03,  1.1420e-01,\n",
      "        -2.5146e-02,  8.3588e-02,  2.2964e-02,  1.1856e-02,  2.4345e-02,\n",
      "        -5.6808e-02, -2.7020e-02,  2.2278e-03, -1.7797e-02,  7.5284e-02,\n",
      "        -4.8828e-02, -6.7767e-02, -8.2355e-02,  1.0712e-02,  3.4454e-02,\n",
      "        -4.5639e-02, -2.9068e-03,  6.2237e-03, -1.5887e-01, -2.0554e-02,\n",
      "         2.0031e-02,  4.7836e-02,  7.2212e-03,  5.4657e-02, -3.8643e-02,\n",
      "         2.8862e-02, -2.8816e-02,  1.0287e-01,  1.4000e-02,  8.0514e-02,\n",
      "        -2.1648e-02, -2.3834e-02,  3.2425e-02, -1.0088e-01, -4.7211e-02,\n",
      "        -4.9610e-02, -3.1586e-02, -7.6019e-02,  1.6769e-02, -1.6556e-02,\n",
      "         1.4045e-01,  4.1924e-03, -2.9903e-02, -9.1131e-02,  2.6840e-02,\n",
      "         3.1235e-02,  1.6611e-01,  1.3245e-01,  1.4622e-01, -3.5217e-02,\n",
      "        -3.7403e-02, -1.8448e-02, -1.0671e-01, -7.3608e-02, -2.8275e-02,\n",
      "         1.4717e-02, -1.0841e-01,  2.7039e-02,  9.1908e-02,  3.1464e-02,\n",
      "        -2.3796e-02, -7.5195e-02, -7.9346e-02,  5.1147e-02,  7.1609e-02,\n",
      "        -4.9865e-02,  3.3905e-02, -1.0410e-01,  5.6381e-03, -6.6910e-02,\n",
      "         7.1594e-02,  9.8434e-02, -1.7883e-02,  3.2745e-02, -9.0027e-04])\n",
      "Sample padded choices: tensor([[ 0.0386,  0.1309, -0.2500,  ..., -0.0931,  0.0560, -0.1223],\n",
      "        [ 0.3184, -0.2598,  0.1963,  ..., -0.2002,  0.0398,  0.0276],\n",
      "        [ 0.2002,  0.0991, -0.1191,  ..., -0.1523, -0.0021, -0.2354],\n",
      "        [ 0.0126, -0.0306, -0.1111,  ..., -0.1099, -0.0348, -0.0258],\n",
      "        [ 0.0522,  0.1616, -0.0900,  ...,  0.0035,  0.1655, -0.2485]])\n",
      "Sample label: tensor(2)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloaders(batch_size=wandb.config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"[LSTM] Sample question shape:\", questions.shape)\n",
    "    print(\"[LSTM] Sample choices shape:\", choices.shape)\n",
    "    print(\"[LSTM] Label:\", labels[0])\n",
    "    break\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"Sample padded question:\", questions[0])\n",
    "    print(\"Sample padded choices:\", choices[0])\n",
    "    print(\"Sample label:\", labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **13. Model Architecture â€“ Two-Layer Feedforward Classifier**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This architecture is based on the first model specified in the project description.\n",
    "- I use a simple two-layer fully connected network to classify questions using pre-trained word embeddings.\n",
    "- A ReLU activation function introduces non-linearity, which improves the modelâ€™s ability to learn complex patterns.\n",
    "- This simpler model serves as a baseline before introducing recurrent components (e.g., LSTM/GRU) in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "INPUT_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 5\n",
    "model = FeedforwardClassifier(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **14. Model Architecture â€“ LSTM Classifier**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This model extends the input representation by modeling temporal relationships between tokens.\n",
    "- Instead of averaging embeddings, it uses an LSTM layer to process the concatenated question and choice embeddings.\n",
    "- For each choice, the LSTM receives the question and choice embeddings concatenated together, and outputs a score for that choice.\n",
    "- The model returns logits over all five choices, which are passed into the loss function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim * 2, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, question_seq, choice_seqs):\n",
    "                # Expand question tensor to match the shape of the choices\n",
    "        question_seq = question_seq.unsqueeze(1).expand(-1, 5, -1)\n",
    "        combined = torch.cat((question_seq, choice_seqs), dim=2)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(combined.size(1)):\n",
    "            choice_input = combined[:, i, :].unsqueeze(1)\n",
    "            lstm_out, _ = self.lstm(choice_input)\n",
    "            output = self.fc(lstm_out[:, -1, :])\n",
    "            outputs.append(output)\n",
    "\n",
    "        logits = torch.stack(outputs, dim=1).squeeze(2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **15. Define Loss Function & Optimizer**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- `CrossEntropyLoss` is standard for multi-class classification tasks like CommonsenseQA.\n",
    "- `Adam` optimizer adapts learning rates and usually performs well with minimal tuning.\n",
    "- The learning rate is stored in WandB config to support reproducible experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **16. Training Loop**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This loop trains either the FFNN or LSTM model depending on the current WandB config.\n",
    "- Tracks training loss and accuracy over epochs.\n",
    "- Logs results to WandB for performance monitoring.\n",
    "- Implements Early Stopping to prevent overfitting when validation performance stagnates.\n",
    "- Saves two model versions: the one with best validation accuracy, and the last seen model in case of interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for questions, choices, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(questions, choices)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        wandb.log({\"train_loss\": total_loss / len(train_loader), \"train_acc\": train_acc})\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for questions, choices, labels in val_loader:\n",
    "                outputs = model(questions, choices)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        wandb.log({\"val_acc\": val_acc})\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            artifact = wandb.Artifact(\"best-model\", type=\"model\")\n",
    "            artifact.add_file(\"best_model.pt\")\n",
    "            wandb.log_artifact(artifact)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # Save last seen model every epoch\n",
    "        torch.save(model.state_dict(), \"last_model.pt\")\n",
    "        wandb.save(\"last_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **17. Hyperparameter Sweep Configuration**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Hyperparameter sweeps allow automated exploration of multiple configurations.\n",
    "- We use a `random` search strategy to efficiently test combinations of architecture, hidden size, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"architecture\": {\"values\": [\"ffnn\", \"lstm\"]},\n",
    "        \"hidden_dim\": {\"values\": [64, 128, 256]},\n",
    "        \"learning_rate\": {\"min\": 1e-4, \"max\": 5e-3}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"commonsense_qa\")\n",
    "\n",
    "# Training entry point for the sweep\n",
    "\n",
    "def sweep_train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    model_class = FeedforwardClassifier if config.architecture == \"ffnn\" else LSTMClassifier\n",
    "    train_loader, val_loader = get_dataloaders()\n",
    "\n",
    "    model = model_class(config.input_dim, config.hidden_dim, config.output_dim)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, loss_function, optimizer)\n",
    "\n",
    "# To run the sweep:\n",
    "# wandb.agent(sweep_id, function=sweep_train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18. Evaluation**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This evaluation phase is run after training to assess the final performance of the model on a validation or test set.\n",
    "- The best saved model (based on validation accuracy) is loaded from disk.\n",
    "- Final accuracy is computed to summarize model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for final evaluation\n",
    "def evaluate_model(model_class, input_dim, hidden_dim, output_dim, val_loader):\n",
    "    model = model_class(input_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for questions, choices, labels in val_loader:\n",
    "            outputs = model(questions, choices)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Final Evaluation Accuracy: {accuracy:.4f}\")\n",
    "    wandb.log({\"final_eval_accuracy\": accuracy})\n",
    "\n",
    "# Example call:\n",
    "# evaluate_model(FeedforwardClassifier, 300, 128, 5, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **19. Tools Used**\n",
    "\n",
    "This project relies on the following tools and libraries:\n",
    "\n",
    "- **PyTorch**: Model building, training, and data utilities\n",
    "- **Hugging Face Datasets**: Loading CommonsenseQA efficiently\n",
    "- **NLTK**: Tokenization and text cleaning\n",
    "- **Gensim**: Pretrained word embeddings (word2vec)\n",
    "- **Weights & Biases (wandb)**: Logging, hyperparameter tracking, and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š **Experiment tracking report:** [View report on WandB](https://wandb.ai/YOUR-USER/YOUR-PROJECT/reports)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
