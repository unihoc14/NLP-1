{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "This notebook serves as the final submission for the project on **Word Embeddings & RNNs** using the **CommonsenseQA dataset**. It includes:\n",
    "\n",
    "- Data loading\n",
    "- Data preprocessing (tokenization, cleaning, padding)\n",
    "- Integration of Word Embeddings (word2vec)\n",
    "- Model definition using a two-layer feedforward classifier\n",
    "- Loss function and optimizer setup\n",
    "- Experiment tracking with Weights & Biases (WandB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Setup & Installations**\n",
    "\n",
    "**Rationale:**\n",
    "- `torch`: Required for model training and tensor processing.\n",
    "- `datasets`: Used to load the CommonsenseQA dataset from Hugging Face.\n",
    "- `nltk`: Employed for tokenization and text preprocessing.\n",
    "- `gensim`: Used for loading pre-trained word embeddings (word2vec).\n",
    "- `torch.nn`: Required to define the model architecture.\n",
    "- `torch.optim`: Used for training the model with gradient updates.\n",
    "- `wandb`: Used for tracking experiments, logging metrics, and visualizing model performance.\n",
    "- `argparse`: Enables dynamic configuration of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\nlp_1\\.venv\\lib\\site-packages (2.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: datasets in c:\\nlp_1\\.venv\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: nltk in c:\\nlp_1\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\nlp_1\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: wandb in c:\\nlp_1\\.venv\\lib\\site-packages (0.19.8)\n",
      "Requirement already satisfied: filelock in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: click in c:\\nlp_1\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\nlp_1\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\nlp_1\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (2.23.1)\n",
      "Requirement already satisfied: setproctitle in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in c:\\nlp_1\\.venv\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: colorama in c:\\nlp_1\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\nlp_1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: wrapt in c:\\nlp_1\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\nlp_1\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\nlp_1\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\nlp_1\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\nlp_1\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install torch datasets nltk gensim wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Loading**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Follows the exact project specification from the Course Project.pdf.\n",
    "- Loads specific dataset splits for training, validation, and test using the TAU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Data Preprocessing**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Prepares raw text inputs for downstream modeling.\n",
    "- Ensures text normalization through lowercasing and punctuation removal.\n",
    "- Tokenization helps break input into word-level units for embeddings.\n",
    "- Padding standardizes input lengths for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Dataset Class (Shared)**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Provides a PyTorch-compatible wrapper around the CommonsenseQA dataset.\n",
    "- Centralizes tokenization, preprocessing, and answer label formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            self.dataset = train\n",
    "        elif split == \"validation\":\n",
    "            self.dataset = valid\n",
    "        elif split == \"test\":\n",
    "            self.dataset = test\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "        self.processed_data = self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        processed = []\n",
    "        for item in self.dataset:\n",
    "            question = preprocess_text(item[\"question\"])\n",
    "            choices = [preprocess_text(choice) for choice in item[\"choices\"][\"text\"]]\n",
    "            answer = ord(item[\"answerKey\"]) - ord('A')\n",
    "            processed.append((question, choices, answer))\n",
    "        return processed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Load Word Embeddings (word2vec)**\n",
    "\n",
    "**Rationale:**\n",
    "- I use Gensim's pre-trained Google News vectors (300D) for semantic information.\n",
    "- Pre-trained embeddings reduce the need for large training data.\n",
    "- Unknown words return a zero vector, which avoids crashes during lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0189,  0.1187, -0.0625,  0.0786, -0.0347,  0.2520,  0.0206, -0.1641,\n",
      "        -0.0212,  0.3184])\n"
     ]
    }
   ],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_word_vector(word):\n",
    "    vec = word2vec[word] if word in word2vec else torch.zeros(300)\n",
    "    return torch.tensor(vec) if not isinstance(vec, torch.Tensor) else vec\n",
    "\n",
    "print(get_word_vector(\"cold\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Feedforward Data Pipeline**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Converts token sequences into averaged embeddings.\n",
    "- Simplifies the input into fixed-size vectors.\n",
    "- Suitable for feedforward networks requiring consistent input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sequence(tokens):\n",
    "    vectors = [get_word_vector(token) for token in tokens]\n",
    "    if len(vectors) == 0:\n",
    "        return torch.zeros(300)\n",
    "    return torch.stack(vectors).mean(dim=0)\n",
    "\n",
    "def collate_fn_ffnn(batch):\n",
    "    questions, choices, answers = zip(*batch)\n",
    "    embedded_questions = torch.stack([embed_sequence(q) for q in questions])\n",
    "    embedded_choices = torch.stack([\n",
    "        torch.stack([embed_sequence(choice) for choice in choice_list])\n",
    "        for choice_list in choices\n",
    "    ])\n",
    "    return embedded_questions, embedded_choices, torch.tensor(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. LSTM Data Pipeline**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- I use full padded sequences of word embeddings for input.\n",
    "- Prepares input suitable for recurrent models (e.g., LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens):\n",
    "    return torch.stack([get_word_vector(token) for token in tokens]) if tokens else torch.zeros((1, 300))\n",
    "\n",
    "def collate_fn_lstm(batch):\n",
    "    questions, choices, answers = zip(*batch)\n",
    "    embedded_questions = torch.nn.utils.rnn.pad_sequence(\n",
    "        [embed_tokens(q) for q in questions], batch_first=True\n",
    "    )\n",
    "    embedded_choices = torch.stack([\n",
    "        torch.nn.utils.rnn.pad_sequence([embed_tokens(choice) for choice in choice_list], batch_first=True)\n",
    "        for choice_list in choices\n",
    "    ])\n",
    "    return embedded_questions, embedded_choices, torch.tensor(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Initialize WandB Tracking**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- WandB helps track hyperparameters, losses, and accuracy during training.\n",
    "- Enables experiment comparison and makes training behavior reproducible.\n",
    "- The architecture type (\"ffnn\" or \"lstm\") is also logged in the configuration to support automatic routing in data loading and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"commonsense_qa\", name=\"ffnn_baseline\")\n",
    "wandb.config.update({\n",
    "    \"architecture\": \"ffnn\",\n",
    "    \"input_dim\": 300,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"output_dim\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 16\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Define get_dataloaders Function**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Automatically chooses the correct collate function based on the model architecture defined in the WandB configuration.\n",
    "- Avoids manual mode switching or hardcoding.\n",
    "- Enables consistent data access for both training and evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=None):\n",
    "    batch_size = batch_size or wandb.config[\"batch_size\"]\n",
    "    mode = wandb.config.get(\"architecture\", \"ffnn\")\n",
    "    collate = collate_fn_ffnn if mode == \"ffnn\" else collate_fn_lstm\n",
    "\n",
    "    train_dataset = CommonsenseQADataset(\"train\")\n",
    "    val_dataset = CommonsenseQADataset(\"validation\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11. Pipeline Test â€“ FFNN**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Ensures the FFNN collate function works and produces expected shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFNN] Sample question shape: torch.Size([16, 300])\n",
      "[FFNN] Sample choices shape: torch.Size([16, 5, 300])\n",
      "[FFNN] Label: tensor(4)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloaders(batch_size=wandb.config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"[FFNN] Sample question shape:\", questions.shape)\n",
    "    print(\"[FFNN] Sample choices shape:\", choices.shape)\n",
    "    print(\"[FFNN] Label:\", labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12. Pipeline Test â€“ LSTM**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Verifies padding and batching for the LSTM collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Sample question shape: torch.Size([16, 300])\n",
      "[LSTM] Sample choices shape: torch.Size([16, 5, 300])\n",
      "[LSTM] Label: tensor(3)\n",
      "Sample padded question: tensor([ 0.0345,  0.0315,  0.0667,  0.0334, -0.0506, -0.0600,  0.0222, -0.0710,\n",
      "         0.0644,  0.0417,  0.0005, -0.1590, -0.0215, -0.0068, -0.0921,  0.0489,\n",
      "         0.0184,  0.0781,  0.0350, -0.0151, -0.0358,  0.0390,  0.0398, -0.0425,\n",
      "         0.0656, -0.0078, -0.0683,  0.0432,  0.0389, -0.0032, -0.0567, -0.0002,\n",
      "        -0.0671, -0.0072,  0.0630,  0.0154,  0.0419, -0.0256,  0.0241,  0.0524,\n",
      "         0.0840, -0.0438,  0.1309, -0.0555, -0.0325,  0.0114, -0.0354,  0.0106,\n",
      "         0.0065,  0.0855,  0.0537,  0.0085, -0.0106,  0.0205,  0.0142,  0.0617,\n",
      "         0.0191, -0.0590,  0.0241, -0.0432, -0.0084,  0.0986, -0.0865, -0.0567,\n",
      "        -0.0437, -0.0032, -0.0225,  0.0847, -0.0150,  0.0560,  0.0397,  0.0359,\n",
      "         0.0739,  0.0368, -0.0995, -0.0134,  0.1043,  0.0723,  0.0387,  0.1294,\n",
      "         0.0524, -0.0771,  0.0451, -0.0131, -0.1286, -0.0406, -0.0704,  0.1278,\n",
      "        -0.0124, -0.0188,  0.0348,  0.0395, -0.0478, -0.0545,  0.0056, -0.0664,\n",
      "        -0.0201,  0.0746, -0.0153, -0.0195, -0.0499, -0.0508,  0.0015,  0.0274,\n",
      "        -0.0246, -0.0270, -0.0577, -0.0562,  0.0244, -0.1108, -0.0394,  0.0426,\n",
      "         0.0067,  0.0137,  0.0360, -0.0102,  0.0924, -0.0738,  0.0996,  0.0318,\n",
      "        -0.1370,  0.0566, -0.0373, -0.0117, -0.0442,  0.0055, -0.0377, -0.0497,\n",
      "         0.0158, -0.0140, -0.0793, -0.1135, -0.0902, -0.0544, -0.0316, -0.0225,\n",
      "         0.0369, -0.0114, -0.0418,  0.0469,  0.0112, -0.0647,  0.0950, -0.0367,\n",
      "        -0.0044,  0.0588, -0.0958, -0.0921, -0.1044, -0.0021,  0.0043,  0.0690,\n",
      "        -0.1223,  0.0566, -0.0080, -0.0204, -0.0880, -0.0976, -0.0057,  0.0432,\n",
      "        -0.0294,  0.0904,  0.0473,  0.0635, -0.0111, -0.0836,  0.0299, -0.0061,\n",
      "         0.0917, -0.0096, -0.1235, -0.0113, -0.0596, -0.0952, -0.0215, -0.0244,\n",
      "         0.0838, -0.0418,  0.0350, -0.0214, -0.0612, -0.0193, -0.0086, -0.0241,\n",
      "         0.0523, -0.0204, -0.0342,  0.0455,  0.0717,  0.0370,  0.0363,  0.0380,\n",
      "         0.0786,  0.0243, -0.0533,  0.0146,  0.0206, -0.0099, -0.0400, -0.1249,\n",
      "         0.0323,  0.0306, -0.0313,  0.0159,  0.0240, -0.0093, -0.0775,  0.0060,\n",
      "         0.0624, -0.0254,  0.0033, -0.0122,  0.0272,  0.0230, -0.1206, -0.0006,\n",
      "         0.1150, -0.0142, -0.1070,  0.0175,  0.0047,  0.0587,  0.0027, -0.0559,\n",
      "         0.1340, -0.0676,  0.0847,  0.0825, -0.0176,  0.0248,  0.0664, -0.0446,\n",
      "        -0.0113, -0.0489,  0.1124, -0.0510, -0.0162, -0.0353,  0.0679, -0.0149,\n",
      "         0.0453, -0.0168,  0.0029, -0.0833, -0.0055,  0.0369,  0.0388,  0.0322,\n",
      "         0.0172,  0.0006,  0.0135,  0.0041,  0.0776,  0.0541,  0.0792, -0.0360,\n",
      "         0.0359,  0.0358, -0.0263, -0.0890,  0.0353, -0.0218, -0.0595,  0.0902,\n",
      "        -0.0031,  0.1364, -0.0080,  0.0190, -0.0417,  0.0455,  0.1104,  0.0751,\n",
      "         0.1482,  0.0958,  0.0739, -0.0247, -0.0259, -0.0970,  0.0022,  0.0160,\n",
      "         0.0275, -0.0354, -0.0142,  0.0478,  0.0079, -0.0423, -0.0366, -0.0422,\n",
      "         0.0462,  0.0832, -0.0716, -0.0051, -0.1103,  0.0059, -0.0653,  0.0316,\n",
      "         0.0410, -0.0458,  0.1138, -0.0734])\n",
      "Sample padded choices: tensor([[-0.0105, -0.0115,  0.0197,  ..., -0.0898,  0.1339, -0.0336],\n",
      "        [ 0.0261,  0.1514, -0.2207,  ..., -0.0732,  0.2197, -0.1396],\n",
      "        [ 0.1475,  0.1396,  0.0640,  ..., -0.1699, -0.0255,  0.2246],\n",
      "        [-0.0120,  0.0297, -0.0824,  ..., -0.0482,  0.0553, -0.0280],\n",
      "        [-0.0823, -0.1121, -0.1741,  ...,  0.0349,  0.0310,  0.0216]])\n",
      "Sample label: tensor(3)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloaders(batch_size=wandb.config[\"batch_size\"])\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"[LSTM] Sample question shape:\", questions.shape)\n",
    "    print(\"[LSTM] Sample choices shape:\", choices.shape)\n",
    "    print(\"[LSTM] Label:\", labels[0])\n",
    "    break\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"Sample padded question:\", questions[0])\n",
    "    print(\"Sample padded choices:\", choices[0])\n",
    "    print(\"Sample label:\", labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **13. Model Architecture â€“ Two-Layer Feedforward Classifier**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This architecture is based on the first model specified in the project description.\n",
    "- I use a simple two-layer fully connected network to classify questions using pre-trained word embeddings.\n",
    "- A ReLU activation function introduces non-linearity, which improves the modelâ€™s ability to learn complex patterns.\n",
    "- This simpler model serves as a baseline before introducing recurrent components (e.g., LSTM/GRU) in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "INPUT_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 5\n",
    "model = FeedforwardClassifier(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **14. Model Architecture â€“ LSTM Classifier**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This model extends the input representation by modeling temporal relationships between tokens.\n",
    "- Instead of averaging embeddings, it uses an LSTM layer to process the concatenated question and choice embeddings.\n",
    "- For each choice, the LSTM receives the question and choice embeddings concatenated together, and outputs a score for that choice.\n",
    "- The model returns logits over all five choices, which are passed into the loss function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim * 2, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, question_seq, choice_seqs):\n",
    "                # Expand question tensor to match the shape of the choices\n",
    "        question_seq = question_seq.unsqueeze(1).expand(-1, 5, -1)\n",
    "        combined = torch.cat((question_seq, choice_seqs), dim=2)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(combined.size(1)):\n",
    "            choice_input = combined[:, i, :].unsqueeze(1)\n",
    "            lstm_out, _ = self.lstm(choice_input)\n",
    "            output = self.fc(lstm_out[:, -1, :])\n",
    "            outputs.append(output)\n",
    "\n",
    "        logits = torch.stack(outputs, dim=1).squeeze(2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **15. Define Loss Function & Optimizer**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- `CrossEntropyLoss` is standard for multi-class classification tasks like CommonsenseQA.\n",
    "- `Adam` optimizer adapts learning rates and usually performs well with minimal tuning.\n",
    "- The learning rate is stored in WandB config to support reproducible experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **16. Training Loop**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This loop trains either the FFNN or LSTM model depending on the current WandB config.\n",
    "- Tracks training loss and accuracy over epochs.\n",
    "- Logs results to WandB for performance monitoring.\n",
    "- Implements Early Stopping to prevent overfitting when validation performance stagnates.\n",
    "- Saves two model versions: the one with best validation accuracy, and the last seen model in case of interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for questions, choices, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(questions, choices)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        wandb.log({\"train_loss\": total_loss / len(train_loader), \"train_acc\": train_acc})\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for questions, choices, labels in val_loader:\n",
    "                outputs = model(questions, choices)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        wandb.log({\"val_acc\": val_acc})\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            artifact = wandb.Artifact(\"best-model\", type=\"model\")\n",
    "            artifact.add_file(\"best_model.pt\")\n",
    "            wandb.log_artifact(artifact)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # Save last seen model every epoch\n",
    "        torch.save(model.state_dict(), \"last_model.pt\")\n",
    "        wandb.save(\"last_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **17. Hyperparameter Sweep Configuration**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Hyperparameter sweeps allow automated exploration of multiple configurations.\n",
    "- We use a `random` search strategy to efficiently test combinations of architecture, hidden size, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"architecture\": {\"values\": [\"ffnn\", \"lstm\"]},\n",
    "        \"hidden_dim\": {\"values\": [64, 128, 256]},\n",
    "        \"learning_rate\": {\"min\": 1e-4, \"max\": 5e-3}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"commonsense_qa\")\n",
    "\n",
    "# Training entry point for the sweep\n",
    "\n",
    "def sweep_train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    model_class = FeedforwardClassifier if config.architecture == \"ffnn\" else LSTMClassifier\n",
    "    train_loader, val_loader = get_dataloaders()\n",
    "\n",
    "    model = model_class(config.input_dim, config.hidden_dim, config.output_dim)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, loss_function, optimizer)\n",
    "\n",
    "# To run the sweep:\n",
    "# wandb.agent(sweep_id, function=sweep_train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18. Evaluation**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This evaluation phase is run after training to assess the final performance of the model on a validation or test set.\n",
    "- The best saved model (based on validation accuracy) is loaded from disk.\n",
    "- Final accuracy is computed to summarize model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for final evaluation\n",
    "def evaluate_model(model_class, input_dim, hidden_dim, output_dim, val_loader):\n",
    "    model = model_class(input_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for questions, choices, labels in val_loader:\n",
    "            outputs = model(questions, choices)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Final Evaluation Accuracy: {accuracy:.4f}\")\n",
    "    wandb.log({\"final_eval_accuracy\": accuracy})\n",
    "\n",
    "# Example call:\n",
    "# evaluate_model(FeedforwardClassifier, 300, 128, 5, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **19. Tools Used**\n",
    "\n",
    "This project relies on the following tools and libraries:\n",
    "\n",
    "- **PyTorch**: Model building, training, and data utilities\n",
    "- **Hugging Face Datasets**: Loading CommonsenseQA efficiently\n",
    "- **NLTK**: Tokenization and text cleaning\n",
    "- **Gensim**: Pretrained word embeddings (word2vec)\n",
    "- **Weights & Biases (wandb)**: Logging, hyperparameter tracking, and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š **Experiment tracking report:** [View report on WandB](https://wandb.ai/YOUR-USER/YOUR-PROJECT/reports)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
