{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in .\\.venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in .\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in .\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in .\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in .\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in .\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in .\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in .\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in .\\.venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in .\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in .\\.venv\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in .\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in .\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in .\\.venv\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in .\\.venv\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\.venv\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in .\\.venv\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in .\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\nlp_1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"commonsense_qa\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '075e483d21c29a511267ef62bedc0461', 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'question_concept': 'punishing', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1afa02df02c908a558b4036e80242fac', 'question': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?', 'question_concept': 'revolving door', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['bank', 'library', 'department store', 'mall', 'new york']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"validation\"][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '90b30172e645ff91f7171a048582eb8b', 'question': 'The townhouse was a hard sell for the realtor, it was right next to a high rise what?', 'question_concept': 'townhouse', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['suburban development', 'apartment building', 'bus stop', 'michigan', 'suburbs']}, 'answerKey': ''}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"test\"][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 9741\n",
      "Validation Size: 1221\n",
      "Test Size: 1140\n",
      "                                 id  \\\n",
      "0  075e483d21c29a511267ef62bedc0461   \n",
      "1  61fe6e879ff18686d7552425a36344c8   \n",
      "2  4c1cb0e95b99f72d55c068ba0255c54d   \n",
      "3  02e821a3e53cb320790950aab4489e85   \n",
      "4  23505889b94e880c3e89cff4ba119860   \n",
      "\n",
      "                                            question question_concept  \\\n",
      "0  The sanctions against the school were a punish...        punishing   \n",
      "1  Sammy wanted to go to where the people were.  ...           people   \n",
      "2  To locate a choker not located in a jewelry bo...           choker   \n",
      "3  Google Maps and other highway and street GPS s...          highway   \n",
      "4  The fox walked from the city into the forest, ...              fox   \n",
      "\n",
      "                                             choices answerKey  \n",
      "0  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         A  \n",
      "1  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         B  \n",
      "2  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         A  \n",
      "3  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         D  \n",
      "4  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         C  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(dataset[\"train\"])\n",
    "df_val = pd.DataFrame(dataset[\"validation\"])\n",
    "df_test = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "print(f\"Train Size: {len(df_train)}\")\n",
    "print(f\"Validation Size: {len(df_val)}\")\n",
    "print(f\"Test Size: {len(df_test)}\")\n",
    "\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'â€“' (U+2013) (4237726314.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    **Final Submission â€“ Data Processing and Model Training**\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character 'â€“' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "**Final Submission â€“ Data Processing and Model Training**\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "This notebook serves as the final submission for the project on **Word Embeddings & RNNs** using the **CommonsenseQA dataset**. It includes:\n",
    "\n",
    "- Data loading\n",
    "- Data preprocessing (tokenization, cleaning, padding)\n",
    "- Integration of Word Embeddings (word2vec)\n",
    "- Model definition using a two-layer feedforward classifier\n",
    "- Loss function and optimizer setup\n",
    "- Experiment tracking with Weights & Biases (WandB)\n",
    "\n",
    "## **2. Setup & Installations**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- `torch`: Required for model training and tensor processing.\n",
    "- `datasets`: Used to load the CommonsenseQA dataset from Hugging Face.\n",
    "- `nltk`: Employed for tokenization and text preprocessing.\n",
    "- `gensim`: Used for loading pre-trained word embeddings (word2vec).\n",
    "- `torch.nn`: Required to define the model architecture.\n",
    "- `torch.optim`: Used for training the model with gradient updates.\n",
    "- `wandb`: Used for tracking experiments, logging metrics, and visualizing model performance.\n",
    "- `argparse`: Enables dynamic configuration of hyperparameters.\n",
    "\n",
    "```python\n",
    "# Install required packages\n",
    "%pip install torch datasets nltk gensim wandb\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import argparse\n",
    "```\n",
    "\n",
    "## **3. Data Loading**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Follows the exact project specification from the Course Project.pdf.\n",
    "- Loads specific dataset splits for training, validation, and test using the TAU version.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "# Check dataset sizes\n",
    "print(len(train), len(valid), len(test))\n",
    "```\n",
    "\n",
    "## **4. Data Preprocessing**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Prepares raw text inputs for downstream modeling.\n",
    "- Ensures text normalization through lowercasing and punctuation removal.\n",
    "- Tokenization helps break input into word-level units for embeddings.\n",
    "- Padding standardizes input lengths for batch processing.\n",
    "\n",
    "```python\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "```\n",
    "\n",
    "## **5. Dataset Class (Shared)**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Provides a PyTorch-compatible wrapper around the CommonsenseQA dataset.\n",
    "- Centralizes tokenization, preprocessing, and answer label formatting.\n",
    "\n",
    "```python\n",
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            self.dataset = train\n",
    "        elif split == \"validation\":\n",
    "            self.dataset = valid\n",
    "        elif split == \"test\":\n",
    "            self.dataset = test\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "        self.processed_data = self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        processed = []\n",
    "        for item in self.dataset:\n",
    "            question = preprocess_text(item[\"question\"])\n",
    "            choices = [preprocess_text(choice) for choice in item[\"choices\"][\"text\"]]\n",
    "            answer = ord(item[\"answerKey\"]) - ord('A')\n",
    "            processed.append((question, choices, answer))\n",
    "        return processed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n",
    "````\n",
    "\n",
    "## **6. Load Word Embeddings (word2vec)**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- I use Gensim's pre-trained Google News vectors (300D) for semantic information.\n",
    "- Pre-trained embeddings reduce the need for large training data.\n",
    "- Unknown words return a zero vector, which avoids crashes during lookup.\n",
    "\n",
    "```python\n",
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_word_vector(word):\n",
    "    return word2vec[word] if word in word2vec else torch.zeros(300)\n",
    "\n",
    "print(get_word_vector(\"cold\")[:10])\n",
    "```\n",
    "\n",
    "## **7. Feedforward Data Pipeline**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Converts token sequences into averaged embeddings.\n",
    "- Simplifies the input into fixed-size vectors.\n",
    "- Suitable for feedforward networks requiring consistent input shape.\n",
    "\n",
    "```python\n",
    "def embed_sequence(tokens):\n",
    "    vectors = [get_word_vector(token) for token in tokens]\n",
    "    if len(vectors) == 0:\n",
    "        return torch.zeros(300)\n",
    "    return torch.stack(vectors).mean(dim=0)\n",
    "\n",
    "def collate_fn_ffnn(batch):\n",
    "    questions, choices, answers = zip(*batch)\n",
    "    embedded_questions = torch.stack([embed_sequence(q) for q in questions])\n",
    "    embedded_choices = torch.stack([\n",
    "        torch.stack([embed_sequence(choice) for choice in choice_list])\n",
    "        for choice_list in choices\n",
    "    ])\n",
    "    return embedded_questions, embedded_choices, torch.tensor(answers)\n",
    "```\n",
    "\n",
    "## **8. LSTM Data Pipeline**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- I use full padded sequences of word embeddings for input.\n",
    "- Prepares input suitable for recurrent models (e.g., LSTM).\n",
    "\n",
    "```python\n",
    "def embed_tokens(tokens):\n",
    "    return torch.stack([get_word_vector(token) for token in tokens]) if tokens else torch.zeros((1, 300))\n",
    "\n",
    "def collate_fn_lstm(batch):\n",
    "    questions, choices, answers = zip(*batch)\n",
    "    embedded_questions = torch.nn.utils.rnn.pad_sequence(\n",
    "        [embed_tokens(q) for q in questions], batch_first=True\n",
    "    )\n",
    "    embedded_choices = torch.stack([\n",
    "        torch.nn.utils.rnn.pad_sequence([embed_tokens(choice) for choice in choice_list], batch_first=True)\n",
    "        for choice_list in choices\n",
    "    ])\n",
    "    return embedded_questions, embedded_choices, torch.tensor(answers)\n",
    "```\n",
    "\n",
    "## **9. Initialize WandB Tracking**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- WandB helps track hyperparameters, losses, and accuracy during training.\n",
    "- Enables experiment comparison and makes training behavior reproducible.\n",
    "- The architecture type (\"ffnn\" or \"lstm\") is also logged in the configuration to support automatic routing in data loading and model selection.\n",
    "\n",
    "```python\n",
    "wandb.init(project=\"commonsense_qa\", name=\"ffnn_baseline\")\n",
    "wandb.config.update({\n",
    "    \"architecture\": \"ffnn\",\n",
    "    \"input_dim\": 300,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"output_dim\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 16\n",
    "})\n",
    "```\n",
    "\n",
    "## **10. Define get_dataloaders Function**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Automatically chooses the correct collate function based on the model architecture defined in the WandB configuration.\n",
    "- Avoids manual mode switching or hardcoding.\n",
    "- Enables consistent data access for both training and evaluation loops.\n",
    "\n",
    "```python\n",
    "def get_dataloaders(batch_size=None):\n",
    "    batch_size = batch_size or wandb.config[\"batch_size\"]\n",
    "    mode = wandb.config.get(\"architecture\", \"ffnn\")\n",
    "    collate = collate_fn_ffnn if mode == \"ffnn\" else collate_fn_lstm\n",
    "\n",
    "    train_dataset = CommonsenseQADataset(\"train\")\n",
    "    val_dataset = CommonsenseQADataset(\"validation\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "```\n",
    "\n",
    "## **11. Pipeline Test â€“ FFNN**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Ensures the FFNN collate function works and produces expected shapes.\n",
    "\n",
    "```python\n",
    "train_loader, val_loader = get_dataloaders(batch_size=wandb.config[\"batch_size\"], mode=\"ffnn\")\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"[FFNN] Sample question shape:\", questions.shape)\n",
    "    print(\"[FFNN] Sample choices shape:\", choices.shape)\n",
    "    print(\"[FFNN] Label:\", labels[0])\n",
    "    break\n",
    "```\n",
    "\n",
    "## **12. Pipeline Test â€“ LSTM**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Verifies padding and batching for the LSTM collate function.\n",
    "\n",
    "```python\n",
    "train_loader, val_loader = get_dataloaders(batch_size=wandb.config[\"batch_size\"], mode=\"lstm\")\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"[LSTM] Sample question shape:\", questions.shape)\n",
    "    print(\"[LSTM] Sample choices shape:\", choices.shape)\n",
    "    print(\"[LSTM] Label:\", labels[0])\n",
    "    break\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    questions, choices, labels = batch\n",
    "    print(\"Sample padded question:\", questions[0])\n",
    "    print(\"Sample padded choices:\", choices[0])\n",
    "    print(\"Sample label:\", labels[0])\n",
    "    break\n",
    "```\n",
    "\n",
    "## **13. Model Architecture â€“ Two-Layer Feedforward Classifier**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This architecture is based on the first model specified in the project description.\n",
    "- I use a simple two-layer fully connected network to classify questions using pre-trained word embeddings.\n",
    "- A ReLU activation function introduces non-linearity, which improves the modelâ€™s ability to learn complex patterns.\n",
    "- This simpler model serves as a baseline before introducing recurrent components (e.g., LSTM/GRU) in future steps.\n",
    "\n",
    "```python\n",
    "class FeedforwardClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "INPUT_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 5\n",
    "model = FeedforwardClassifier(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "```\n",
    "\n",
    "## **14. Model Architecture â€“ LSTM Classifier**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This model extends the input representation by modeling temporal relationships between tokens.\n",
    "- Instead of averaging embeddings, it uses an LSTM layer to process the concatenated question and choice embeddings.\n",
    "- For each choice, the LSTM receives the question and choice embeddings concatenated together, and outputs a score for that choice.\n",
    "- The model returns logits over all five choices, which are passed into the loss function for training.\n",
    "\n",
    "```python\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim * 2, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, question_seq, choice_seqs):\n",
    "                # Expand question tensor to match the shape of the choices\n",
    "        question_seq = question_seq.unsqueeze(1).expand(-1, 5, -1)\n",
    "        combined = torch.cat((question_seq, choice_seqs), dim=2)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(combined.size(1)):\n",
    "            choice_input = combined[:, i, :].unsqueeze(1)\n",
    "            lstm_out, _ = self.lstm(choice_input)\n",
    "            output = self.fc(lstm_out[:, -1, :])\n",
    "            outputs.append(output)\n",
    "\n",
    "        logits = torch.stack(outputs, dim=1).squeeze(2)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "## **15. Define Loss Function & Optimizer**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- `CrossEntropyLoss` is standard for multi-class classification tasks like CommonsenseQA.\n",
    "- `Adam` optimizer adapts learning rates and usually performs well with minimal tuning.\n",
    "- The learning rate is stored in WandB config to support reproducible experimentation.\n",
    "\n",
    "```python\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])\n",
    "```\n",
    "\n",
    "## **16. Training Loop**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This loop trains either the FFNN or LSTM model depending on the current WandB config.\n",
    "- Tracks training loss and accuracy over epochs.\n",
    "- Logs results to WandB for performance monitoring.\n",
    "- Implements Early Stopping to prevent overfitting when validation performance stagnates.\n",
    "- Saves two model versions: the one with best validation accuracy, and the last seen model in case of interruption.\n",
    "\n",
    "```python\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=10, patience=3):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for questions, choices, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(questions, choices)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        wandb.log({\"train_loss\": total_loss / len(train_loader), \"train_acc\": train_acc})\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for questions, choices, labels in val_loader:\n",
    "                outputs = model(questions, choices)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        wandb.log({\"val_acc\": val_acc})\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            artifact = wandb.Artifact(\"best-model\", type=\"model\")\n",
    "            artifact.add_file(\"best_model.pt\")\n",
    "            wandb.log_artifact(artifact)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # Save last seen model every epoch\n",
    "        torch.save(model.state_dict(), \"last_model.pt\")\n",
    "        wandb.save(\"last_model.pt\")\n",
    "```\n",
    "\n",
    "## **17. Hyperparameter Sweep Configuration**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- Hyperparameter sweeps allow automated exploration of multiple configurations.\n",
    "- We use a `random` search strategy to efficiently test combinations of architecture, hidden size, and learning rate.\n",
    "\n",
    "```python\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"architecture\": {\"values\": [\"ffnn\", \"lstm\"]},\n",
    "        \"hidden_dim\": {\"values\": [64, 128, 256]},\n",
    "        \"learning_rate\": {\"min\": 1e-4, \"max\": 5e-3}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"commonsense_qa\")\n",
    "\n",
    "# Training entry point for the sweep\n",
    "\n",
    "def sweep_train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    model_class = FeedforwardClassifier if config.architecture == \"ffnn\" else LSTMClassifier\n",
    "    train_loader, val_loader = get_dataloaders()\n",
    "\n",
    "    model = model_class(config.input_dim, config.hidden_dim, config.output_dim)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, loss_function, optimizer)\n",
    "\n",
    "# To run the sweep:\n",
    "# wandb.agent(sweep_id, function=sweep_train, count=10)\n",
    "```\n",
    "\n",
    "## **18. Evaluation**\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- This evaluation phase is run after training to assess the final performance of the model on a validation or test set.\n",
    "- The best saved model (based on validation accuracy) is loaded from disk.\n",
    "- Final accuracy is computed to summarize model quality.\n",
    "\n",
    "```python\n",
    "# Load the best model for final evaluation\n",
    "def evaluate_model(model_class, input_dim, hidden_dim, output_dim, val_loader):\n",
    "    model = model_class(input_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for questions, choices, labels in val_loader:\n",
    "            outputs = model(questions, choices)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Final Evaluation Accuracy: {accuracy:.4f}\")\n",
    "    wandb.log({\"final_eval_accuracy\": accuracy})\n",
    "\n",
    "# Example call:\n",
    "# evaluate_model(FeedforwardClassifier, 300, 128, 5, val_loader)\n",
    "```\n",
    "\n",
    "## **19. Tools Used**\n",
    "\n",
    "This project relies on the following tools and libraries:\n",
    "\n",
    "- **PyTorch**: Model building, training, and data utilities\n",
    "- **Hugging Face Datasets**: Loading CommonsenseQA efficiently\n",
    "- **NLTK**: Tokenization and text cleaning\n",
    "- **Gensim**: Pretrained word embeddings (word2vec)\n",
    "- **Weights & Biases (wandb)**: Logging, hyperparameter tracking, and visualizations\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Š **Experiment tracking report:** [View report on WandB](https://wandb.ai/YOUR-USER/YOUR-PROJECT/reports)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
